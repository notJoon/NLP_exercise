{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/not_joon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from matplotlib import pyplot as plt\n",
    "import nltk \n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r'/Users/not_joon/NLP_exercise/data/reviews/reviews.csv'\n",
    "\n",
    "reviews = pd.read_csv(PATH)\n",
    "reviews.drop(reviews.columns[[0,1,2,3,4,5,6,7,8]], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I got a wild hair for taffy and ordered this f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>This saltwater taffy had great flavors and was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This taffy is so good.  It is very soft and ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Right now I'm mostly just sprouting this so my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This is a very healthy dog food. Good for thei...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text\n",
       "0  I have bought several of the Vitality canned d...\n",
       "1  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2  This is a confection that has been around a fe...\n",
       "3  If you are looking for the secret ingredient i...\n",
       "4  Great taffy at a great price.  There was a wid...\n",
       "5  I got a wild hair for taffy and ordered this f...\n",
       "6  This saltwater taffy had great flavors and was...\n",
       "7  This taffy is so good.  It is very soft and ch...\n",
       "8  Right now I'm mostly just sprouting this so my...\n",
       "9  This is a very healthy dog food. Good for thei..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def split_sentences(reviews):\n",
    "    n_reviews = len(reviews)\n",
    "\n",
    "    for i in range(n_reviews):\n",
    "        review = reviews[i]\n",
    "        sentences = sent_tokenize(review)\n",
    "\n",
    "        for j in reversed(range(len(sentences))):\n",
    "            sent = sentences[j]\n",
    "            sentences[j] = sent.strip()\n",
    "\n",
    "            if sent == '':\n",
    "                sentences.pop(j)\n",
    "\n",
    "        reviews[i] = sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_list = list(reviews['Text'])\n",
    "split_sentences(rev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['sent_tokens'] = rev_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['length'] = reviews['sent_tokens'].map(lambda x: len(x))\n",
    "\n",
    "find_length = 5\n",
    "reviews = reviews[reviews['length'] > find_length]\n",
    "\n",
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "## 리뷰들의 최대 vocab의 수를 5000개로 한정\n",
    "MAX_FEATURES = 5000\n",
    "\n",
    "list_sentences_train = reviews['Text']\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "\n",
    "maxlen = 200\n",
    "X_train = pad_sequences(list_tokenized_train, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc \n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "def load_embedding_matrix(file_name: str) -> dict:\n",
    "    \"\"\"\n",
    "    사전 학습된 glove 임베딩 \n",
    "    \"\"\"\n",
    "    EMBEDDING = r'/Users/not_joon/NLP_exercise/data/glove/glove.twitter.27B.25d.txt'\n",
    "    embed_size = 25\n",
    "    \n",
    "    if file_name=='glove':\n",
    "        embeddings_idx = dict()\n",
    "\n",
    "        f = open(EMBEDDING, encoding='utf-8')\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_idx[word] = coefs ## 50 dims\n",
    "        f.close()\n",
    "\n",
    "        print(f'loaded {len(embeddings_idx)} word vectors')\n",
    "    \n",
    "    gc.collect()\n",
    "    return embeddings_idx\n",
    "\n",
    "\n",
    "\n",
    "def calculate_sentence_embedding(word_list, emb_idx):\n",
    "    \"\"\"\n",
    "    전체 문장에 대해 단어들의 임베딩 평균값으로 문장 임베딩 계산 \n",
    "    \"\"\"\n",
    "    emb_list = []\n",
    "\n",
    "    for k in word_list:\n",
    "        embedding_vec = emb_idx.get(k)\n",
    "\n",
    "        if embedding_vec is not None:\n",
    "            if(len(embedding_vec) == 25):\n",
    "                emb_list.append(list(embedding_vec))\n",
    "    mean_arr = np.array(emb_list)\n",
    "\n",
    "    return np.mean(mean_arr, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "def get_sent_embedding(get_list):\n",
    "    \"\"\"\n",
    "    위에 정의된 함수들을 이용. 문장에 대한 임베딩을 활성화, 전처리 과정을 거침\n",
    "    \"\"\"\n",
    "    sent_emb = []\n",
    "    n_sentences = len(get_list)\n",
    "\n",
    "    for i in get_list:\n",
    "        i = i.lower()\n",
    "        wL = re.sub(\"[^\\w]\", \" \", i).split()\n",
    "\n",
    "        if len(wL) > 0:\n",
    "            for k in wL:\n",
    "                if k in string.punctuation:\n",
    "                    wL.remove(k)\n",
    "            if len(wL) <= 2:\n",
    "                continue\n",
    "        \n",
    "        else:\n",
    "            print(f\"sentence removed: {i}\")\n",
    "            continue\n",
    "            \n",
    "        res = list(calculate_sentence_embedding(wL))\n",
    "        sent_emb.append(res)\n",
    "\n",
    "    return np.array(sent_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_idx = load_embedding_matrix('glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "size = 5000 \n",
    "summary = [None] * size \n",
    "\n",
    "for r in range(size):\n",
    "    review = reviews['sent_tokens'].iloc[r]\n",
    "    encoding = get_sent_embedding(review)\n",
    "\n",
    "    if len(encoding) > 0:\n",
    "        clusters = int(np.ceil(len(encoding) ** 0.5))\n",
    "        kmeans = KMeans(n_clusters=clusters, random_state=42)\n",
    "        kmeans = kmeans.fit(encoding)\n",
    "\n",
    "        avg = []\n",
    "        closest = []\n",
    "\n",
    "        for j in range(clusters):\n",
    "            idx = np.where(kmeans.labels_ == j)[0]\n",
    "            avg.append(np.mean(idx))\n",
    "        \n",
    "        closest, _ = pairwise_distances_argmin_min(\n",
    "            kmeans.cluster_centers_,\n",
    "            encoding\n",
    "        )\n",
    "\n",
    "        ordering = sorted(range(clusters), key=lambda k: avg[k])\n",
    "        summary[r] = ' '.join([reviews[closest[idx]] for idx in ordering])\n",
    "\n",
    "        print(f'Done for reviews # = {r}')\n",
    "    \n",
    "    else:\n",
    "        print(\"not valid\")\n",
    "\n",
    "reviews = reviews[:size]\n",
    "reviews['PredictedSummary'] = summary\n",
    "reviews[['Text', 'PredictedSummary']].to_csv('top_5000_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e3fef3de332457217819c87cd6c3ac7b413b0ac65c12f5257546323ecdf3f8ff"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
