{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📄 문서 요약 (Test Summarization)\n",
    "### 🗒 NOTE: 문서요약\n",
    "텍스트의 의미를 유지하면서 텍스트의 내용을 간략하게 줄이는 것.\n",
    "\n",
    "<p align='center'>\n",
    "    <img src = \"/Users/not_joon/NLP_exercise/images/summarization.png\">\n",
    "</p>\n",
    "\n",
    "###  문서 요약의 과정\n",
    "1. <b> I </b> (interpretation) \n",
    "    - 주어진 문자 텍스트를 해석하여 컴퓨터가 이해할 수 있도록 표현.\n",
    "2. <b> T </b> (transformation)\n",
    "    - 문서 표현을 요약문으로 표현 될 수 있도록 변형(가공)하는 것.\n",
    "3. <b> G </b> (generation)\n",
    "    - 요약문에 대한 표현으로부터 최종 요약문을 생성하는 것  \n",
    "\n",
    "### 입력 문서\n",
    "1. <b>사이즈</b> : 얼마나 많은 양의 문서들을 입력으로 사용\n",
    "2. <b>도메인</b> : 특정 분야를 다루는지 아님 일반적인(general) 목적을 수행하는지 \n",
    "3. <b>형태</b> : 구조적(structure)인 특성 / 작은 규모(low scale) / 새로운(novel) / 텍스트 / 멀티미디어 ...\n",
    "---\n",
    "### 목적\n",
    "#### 사용자 \n",
    " - <b>질의 지향</b>(query-oriented): 사용자의 선호도를 고려한 요약 시스템\n",
    " - <b>일반적</b>(generic): 입력 문서 전반에 대해 무엇을 표현하는지, 저자가 유도한 토픽에 집중\n",
    "\n",
    "#### 사용 용도\n",
    " - informative : 원본의 필수적인 정보를 포함. 사용자가 글을 읽은 이후에 문서의 주요 아이디어가 무엇인지 알 수 있어야 함.\n",
    " - indicative : 유익한 정보는 없고, 원본 문서의 전반적인 설명만을 포함. \n",
    "\n",
    "#### 확장성(expansiveness)\n",
    " 생성된 요약문이 원본 문서의 배경(background)나 일부 과거 문서들과 비교해서 새로운 정보를 제공하는 것. \n",
    " \n",
    " ---\n",
    "### 출력 문서\n",
    "#### 파생\n",
    "요약을 어떤 방식으로 유도하여 파생할지에 광한 것.\n",
    "1. 추출(extraction)\n",
    "    - 통계적 특징을 고려해서 추출된 문장을 정렬하여 요약을 작성\n",
    "    - 텍스트에서 가장 두드러진 문장을 찾는 기술 \n",
    "2. 추상(abstraction)\n",
    "    - 같은 의미의 다른 표현(paraphrasing)을 사용 또는 새로운 단어를 사용하여 요약 \n",
    "    - 일부 추론(reasoning) 및 심픙 분석(deep analyze) 방법들이 이용됨.\n",
    "\n",
    "#### 편파성\n",
    "1. 중립적 : 입력 문서에 대한 어떤 평가나 판단 없이 요약에 반영.\n",
    "2. 평가가 필요\n",
    "    - 명시적(explicit) : 의견이라 판단되면 명시적 판단이라고 결론\n",
    "    - 묵시적(implicit) : 편향을 이용해 일부 자료만 이용하고 나머지는 생략. \n",
    "---\n",
    "### 🔎 접근법\n",
    "#### 통계적 접근법\n",
    "> 텍스트간(문장단위)의 연관성 점수를 계산하는 방식으로 요약문을 구성되며 점수가 높은 문장들이 요약문으로 사용됨. \n",
    "\n",
    "#### `tf-idf`\n",
    " $$idf(t)=log(\\frac{|D|}{|\\frac{d}{tINd}|+1})$$\n",
    "- $|D|$: 코퍼스 $D$에서 문서들의 개수\n",
    "- $|d/tINd|$: $t$를 포함하고 있는 문서들의 개수\n",
    "---\n",
    "#### `텍스트의 위치`\n",
    "단어들과 문장들의 위치는 문서내에 중요한 정보를 파악하기에 잠재적인 조건을 가지고 있다. [(Luhn, 1958)](https://ieeexplore.ieee.org/abstract/document/5392672)\n",
    "예를 들어, 문서내에서 첫 번째와 마지막 문장은 다른 위치에 있는 문장들 보다 중요도가 높은 경향이 있다. \n",
    "\n",
    "<p align='center'>\n",
    "    <img src = \"/Users/not_joon/NLP_exercise/images/luhn score.png\">\n",
    "</p>\n",
    "\n",
    "1. Direct Proportion(DP)\n",
    "    * 처음 출현 -> 점수 1점\n",
    "    * 마지막에 출현 -> $1/n$ ($n$: 문장내의 단어의 개수)\n",
    "\n",
    "2. Inverse Proportion(IP)\n",
    "    * $1/i$ ($i$: 위치 번호)\n",
    "    * 선행되는 문장 위치가 유리\n",
    "\n",
    "3. Geometric Sequences(GS)\n",
    "    * 단어에 대한 점수를 모든 단어에 대해 $(\\frac{1}{2})^{i-1}$로 도출한 값들의 합을 사용\n",
    "\n",
    "4. Binary Function(BF)\n",
    "    * 단어의 첫 등장에는 더 중요한 가중치를 부여 다른 것은 덜 중요하게\n",
    "    * 첫 등장 -> 점수 1점\n",
    "    * 다른 경우 -> $\\lambda\\ll 1$\n",
    "\n",
    "5. 최종 점수\n",
    "\n",
    "$$Score(s) = \\sum_{w_iINs}\\frac{log(freq(w_i))*pos(w_i)}{|s|}$$\n",
    "* $pos(w_i)$: 네 가지 함수들 중 하나\n",
    "* $freq(w_i)$: 단어 $w_i$의 빈도\n",
    "* $|s|$: 문장의 길이\n",
    "---\n",
    "#### 잠재 의미 분석 (LSA)\n",
    "문서들과 문서들에 포함된 단어등 사이의 관계성을 탐색\n",
    "\n",
    "문서내의 용어들을 $m$개의 행(row), $n$개의 열(col)을 문장으로 구성하며 행렬 $A$를 생성\n",
    "$$A=U\\Sigma V^T$$\n",
    "* $U=[u_{i,j}]$: $m \\times n$ 직교행렬 대각 행렬(cloumn-orthonormal)의 열. 즉 왼쪽 특이 벡터\n",
    "* $\\Sigma=diag(\\sigma_1, \\sigma_2, ..., \\sigma_n)$:  $n \\times n$ 대각 행렬(diagonal matrix)의 대각 원소. 내림차순으로 정렬된 음수가 아닌 특이값 벡터(singular vector)로 구성\n",
    "* $V=[v_{i,j}]$: $n \\times n$ 직교 행렬(othogonal vector)의 열, 즉 오른쪽 특이 벡터(right singular vector)로 구성\n",
    "\n",
    "#### 중요도 \n",
    "$$ s_k = \\sqrt{\\sum_{i=1}^{n}v^{2_{k,i}}\\cdot \\sigma^2 } $$\n",
    "* $s_k$: 문장 $k$\n",
    "---\n",
    "#### 기계학습 접근법\n",
    "일반적으로 다른 접근법들과 함께 사용하여 성능을 향상시키는 방향으로 활용된다.\n",
    "#### 결정 함수\n",
    "* 베이지안 분류와 같은 알고리즘을 이용.\n",
    "* 주어진 문장들에 대해 요약문에 포함될지에 대한 확률값을 구함 -> 분류 분제로 활용\n",
    "$$P(s_iINS|\\overrightarrow{f})=\\frac{\\prod_{j=1}^{\\overrightarrow{f}}P(f_j|s_iINS)*P(s_iINS)}{\\prod_{j=1}^{|\\overrightarrow{f}|}P(f_j)}$$\n",
    "* $P(s_iINS)$는 상수값\n",
    "* $P(f_j|s_iINS)$와 $P(f_j)$는 코퍼스로 부터 추정된 값\n",
    "---\n",
    "### 평가\n",
    "#### `ROGUE`(Recall-Oriented Understudy for Gisting Evaluation)\n",
    "정답 요약(reference summary)과 모델이 생성한 요약을 비교해서 성능을 측정함.\n",
    "시스템이 만들어낸 요약과 정답 요약, recall 값에 대한 단위(N-gram)의 개수를 계산하는 방식 \n",
    "* `ROGUE-N`: 고려하는 단위의 길이에 대한 척도 \n",
    "$$ROGUE-(N)=\\frac{\\sum_{S\\in\\sum_{m_{ref}}}\\sum_{N-gramINS}Count_{match(N-gram)}}{\\sum_{S\\in \\sum_{n_{ref}}\\sum_{N-gramINS}Count(N-gram)}}$$\n",
    "* $N$: $N-gram$의 사이즈\n",
    "* $Count_{match}(N-gram)$: 후보 요약과 정답 요약에서 찾은 $N-gram$의 개수\n",
    "* $Count(N-gram)$: 정답 요약에 있는 $N-gram$의 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
